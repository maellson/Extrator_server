import os
import json
import pytesseract
from pdf2image import convert_from_path
import fitz  # PyMuPDF
from dotenv import load_dotenv


# imports atualizados
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

# Carregar vari√°veis de ambiente
load_dotenv()

# Configura√ß√£o do OCR (ajuste o caminho se necess√°rio)
pytesseract.pytesseract.tesseract_cmd = "/opt/homebrew/bin/tesseract"

# Caminhos
pdf_path = "arquivos_pdf/documento.pdf"
images_dir = "imagens_paginas"
texts_dir = "textos_paginas"

os.makedirs(images_dir, exist_ok=True)
os.makedirs(texts_dir, exist_ok=True)


def extrair_texto_via_ocr(pdf_path):
    paginas = convert_from_path(pdf_path, dpi=300)
    textos = []
    for num, pagina in enumerate(paginas, start=1):
        img_path = os.path.join(images_dir, f"pagina_{num}.png")
        pagina.save(img_path, "PNG")

        texto = pytesseract.image_to_string(img_path, lang="por")
        textos.append((num, texto))
        with open(os.path.join(texts_dir, f"pagina_{num}.txt"), "w", encoding="utf-8") as f:
            f.write(texto)
        print(f"[OCR] P√°gina {num} extra√≠da.")
    return textos


def extrair_texto_vetorial(pdf_path):
    doc = fitz.open(pdf_path)
    textos = []
    for num, page in enumerate(doc, start=1):
        texto = page.get_text().strip()
        if texto:
            textos.append((num, texto))
            with open(os.path.join(texts_dir, f"vetorial_pagina_{num}.txt"), "w", encoding="utf-8") as f:
                f.write(texto)
            print(f"[Vetorial] P√°gina {num} extra√≠da.")
    return textos


def extrair_texto_completo(pdf_path):
    ocr = dict(extrair_texto_via_ocr(pdf_path))
    vet = dict(extrair_texto_vetorial(pdf_path))
    todas = {**ocr, **vet}
    return sorted(todas.items())


def criar_vector_store(paginas_texto):
    docs = [f"P√°gina {num}:\n{txt}" for num, txt in paginas_texto]
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=100)
    chunks = splitter.create_documents(docs)

    emb = OpenAIEmbeddings()
    store = FAISS.from_documents(chunks, emb)
    store.save_local("faiss_index")
    print("[FAISS] √çndice salvo em ./faiss_index")


def consultar_vector_store(pergunta: str):
    emb = OpenAIEmbeddings()
    store = FAISS.load_local(
        "faiss_index", emb, allow_dangerous_deserialization=True
    )
    qa = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model="gpt-4o-mini", temperature=0),
        retriever=store.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
    )
    # usa .invoke() em v0.2+
    return qa.invoke({"query": pergunta})


def classificar_documento(paginas_texto):
    conteudo = "\n\n".join(f"P√°gina {n}:\n{t}" for n, t in paginas_texto)

    prompt_template = PromptTemplate.from_template("""
        Voc√™ est√° analisando um documento dividido por p√°ginas, onde cada p√°gina come√ßa com 'P√°gina X:'.
        Classifique as p√°ginas de acordo com o conte√∫do apresentado. Os tipos de conte√∫do que devem ser detectados s√£o:
        - email
        - boleto
        - nota_fiscal

        Considere:
        - 'email' cont√©m sauda√ß√µes, mensagens, assinaturas, ou linguagem de comunica√ß√£o.
        - 'boleto' cont√©m c√≥digo de barras, vencimento, valor, cedente ou banco.
        - 'nota_fiscal' cont√©m CNPJ, descri√ß√£o de produtos/servi√ßos, impostos, natureza da opera√ß√£o.

        Responda apenas com um JSON, nesse formato:
        {{  
        "email": [1, 2],  
        "boleto": [3],  
        "nota_fiscal": [4, 5]  
        }}

        Aqui est√° o conte√∫do do documento:

        {conteudo}
""")

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    chain = prompt_template | llm
    ai_message = chain.invoke({"conteudo": conteudo})

    # Extrai o texto puro do AIMessage
    text = ai_message.content if hasattr(
        ai_message, "content") else str(ai_message)

    # Limpeza de markdown/backticks
    resposta_limpa = text.strip().lstrip("```json").rstrip("```").strip()

    try:
        return json.loads(resposta_limpa)
    except json.JSONDecodeError:
        print("‚ö†Ô∏è Falha ao parsear JSON:")
        print(text)
        return {"erro": "formato inv√°lido", "raw": text}


if __name__ == "__main__":
    print("üìÑ Extraindo texto...")
    paginas = extrair_texto_completo(pdf_path)

    print("üß† Criando embeddings e FAISS index...")
    criar_vector_store(paginas)

    print("üîñ Classificando p√°ginas por tipo:")
    classificacao = classificar_documento(paginas)
    print(json.dumps(classificacao, indent=2))

    print("ü§ñ Fazendo consulta RAG:")
    query = (
        "Voc√™ est√° analisando um documento PDF dividido por p√°ginas. "
        "Cada p√°gina come√ßa com 'P√°gina X:'. "
        "Em qual(is) p√°gina(s) aparece uma nota fiscal? "
        "Responda como: nota_fiscal: [X-Y]"
    )
    resultado = consultar_vector_store(query)

    print("\nüîé Resposta RAG:")
    print(resultado["result"])
    print("\nüìë Fontes:")
    for doc in resultado["source_documents"]:
        print(doc.page_content[:200], "...\n")
